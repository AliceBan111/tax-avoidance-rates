{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bba937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from xgboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from xgboost) (1.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6307116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from lightgbm) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from lightgbm) (1.15.2)\n",
      "Using cached lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459af669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: graphviz in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (3.10.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (1.15.2)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\.conda\\envs\\myenv\\lib\\site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-2.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 3.7/102.4 MB 25.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 11.3/102.4 MB 32.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 22.0/102.4 MB 40.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 29.6/102.4 MB 39.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 44.6/102.4 MB 46.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 59.8/102.4 MB 51.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 75.5/102.4 MB 54.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 90.4/102.4 MB 56.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 58.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 102.4/102.4 MB 55.1 MB/s eta 0:00:00\n",
      "Downloading plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 9.9/9.9 MB 65.6 MB/s eta 0:00:00\n",
      "Downloading narwhals-2.12.0-py3-none-any.whl (425 kB)\n",
      "Installing collected packages: narwhals, plotly, catboost\n",
      "Successfully installed catboost-1.2.8 narwhals-2.12.0 plotly-6.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboostlss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c01c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849cfb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from xgboostlss.model import *\n",
    "from xgboostlss.distributions.Gaussian import *\n",
    "from xgboostlss.datasets.data_loader import load_simulated_gaussian_data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.stats import norm, poisson, gamma\n",
    "\n",
    "import optuna\n",
    "from functools import partial\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import linearmodels as lm\n",
    "from linearmodels import PanelOLS, RandomEffects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a7c77",
   "metadata": {},
   "source": [
    "# Part 1: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f50ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3993, 115)\n",
      "Index(['Ticker', 'Nazwa2', 'rok', 'ta', 'txt', 'pi', 'str', 'xrd', 'ni',\n",
      "       'ppent',\n",
      "       ...\n",
      "       'intan_ma', 'ppe_ma', 'sale_ma', 'cash_holdings_ma', 'roa_past',\n",
      "       'lev_past', 'intan_past', 'ppe_past', 'sale_past',\n",
      "       'cash_holdings_past'],\n",
      "      dtype='object', length=115)\n",
      "          Ticker             Nazwa2   rok         ta      txt         pi  \\\n",
      "0  11B PW Equity  11 bit studios SA  2005  21.127613  1.24185   6.329725   \n",
      "1  11B PW Equity  11 bit studios SA  2006  21.127613  1.24185   6.329725   \n",
      "2  11B PW Equity  11 bit studios SA  2007  21.127613  1.24185   6.329725   \n",
      "3  11B PW Equity  11 bit studios SA  2008  21.127613  1.24185   6.329725   \n",
      "4  11B PW Equity  11 bit studios SA  2009  21.127613  1.24185   6.329725   \n",
      "5  11B PW Equity  11 bit studios SA  2010   1.352400 -0.05370  -0.284900   \n",
      "6  11B PW Equity  11 bit studios SA  2011   2.986200  0.29240   1.585100   \n",
      "7  11B PW Equity  11 bit studios SA  2012   7.336500  0.34030   1.685000   \n",
      "8  11B PW Equity  11 bit studios SA  2013   8.712700  0.27590   1.284300   \n",
      "9  11B PW Equity  11 bit studios SA  2014  21.515900  2.19100  11.426700   \n",
      "\n",
      "    str  xrd      ni     ppent  ...  intan_ma    ppe_ma   sale_ma  \\\n",
      "0  0.19  0.0  5.0879  0.276275  ...  0.198598  0.013076  0.445954   \n",
      "1  0.19  0.0  5.0879  0.276275  ...  0.198598  0.013076  0.445954   \n",
      "2  0.19  0.0  5.0879  0.276275  ...  0.198598  0.013076  0.445954   \n",
      "3  0.19  0.0  5.0879  0.276275  ...  0.198598  0.013076  0.445954   \n",
      "4  0.19  0.0  5.0879  0.276275  ...  0.198598  0.013076  0.445954   \n",
      "5  0.19  0.0 -0.2312  0.002200  ...  0.198598  0.013076  0.445954   \n",
      "6  0.19  0.0  1.2928  0.002200  ...  0.599299  0.007352  0.223347   \n",
      "7  0.19  0.0  1.3447  0.007200  ...  0.508991  0.001182  0.292123   \n",
      "8  0.19  0.0  1.0084  0.063200  ...  0.011029  0.000859  0.479654   \n",
      "9  0.19  0.0  9.2357  0.103600  ...  0.028820  0.004118  0.388653   \n",
      "\n",
      "   cash_holdings_ma  roa_past  lev_past  intan_past  ppe_past  sale_past  \\\n",
      "0          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "1          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "2          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "3          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "4          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "5          0.574744  0.240818       0.0    0.198598  0.013076   0.445954   \n",
      "6          0.557928 -0.170955       0.0    1.000000  0.001627   0.000739   \n",
      "7          0.488124  0.432925       0.0    0.017983  0.000737   0.583506   \n",
      "8          0.453777  0.183289       0.0    0.004076  0.000981   0.375801   \n",
      "9          0.450896  0.115739       0.0    0.053565  0.007254   0.401505   \n",
      "\n",
      "   cash_holdings_past  \n",
      "0            0.574744  \n",
      "1            0.574744  \n",
      "2            0.574744  \n",
      "3            0.574744  \n",
      "4            0.574744  \n",
      "5            0.574744  \n",
      "6            0.541112  \n",
      "7            0.435135  \n",
      "8            0.472419  \n",
      "9            0.429373  \n",
      "\n",
      "[10 rows x 115 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3993 entries, 0 to 4354\n",
      "Columns: 115 entries, Ticker to cash_holdings_past\n",
      "dtypes: float64(57), int64(56), object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train_fe.csv', index_col=0)\n",
    "print(df_train.shape)\n",
    "print(df_train.columns)\n",
    "print(df_train.head(10))\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05dfca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363, 115)\n",
      "            Ticker                        Nazwa2   rok            ta  \\\n",
      "11   11B PW Equity             11 bit studios SA  2016  4.564940e+01   \n",
      "23   1AT PW Equity                Atal SA/Poland  2016  1.513553e+03   \n",
      "35   4FM PW Equity                 4Fun Media SA  2016  4.166300e+01   \n",
      "47   AAL LN Equity            Anglo American PLC  2016  5.014900e+04   \n",
      "59   ABC PW Equity                   ABC Data SA  2016  1.252895e+03   \n",
      "71   ABE PW Equity                         AB SA  2016  1.045349e+03   \n",
      "83   ABF LN Equity  Associated British Foods PLC  2016  1.137600e+04   \n",
      "95   ABS PW Equity  Asseco Business Solutions SA  2016  3.016820e+02   \n",
      "107   AC FP Equity                      Accor SA  2016  1.382241e+06   \n",
      "119  ACG PW Equity                         Ac SA  2016  1.311210e+02   \n",
      "\n",
      "             txt           pi     str     xrd            ni         ppent  \\\n",
      "11      2.801500    15.730800  0.1900     0.0     12.929300      0.779500   \n",
      "23     15.915000   109.042999  0.1900     0.0     89.442001      7.480000   \n",
      "35      0.320200     1.489200  0.1900     0.0      1.346000      5.604800   \n",
      "47    698.000000  2624.000000  0.2000  1594.0  28719.000000     63.000000   \n",
      "59      5.019000    22.233000  0.1900     0.0     17.135000     10.229000   \n",
      "71     11.427334    52.442334  0.1900     0.0     41.004000    101.406917   \n",
      "83    221.000000  1042.000000  0.2000   818.0   5145.000000     36.000000   \n",
      "95     10.033000    52.479000  0.1900     0.0     42.445999     11.721000   \n",
      "107  1969.000000  6307.000000  0.3443     0.0   3874.000000  20066.000000   \n",
      "119     7.526000    37.730000  0.1900     0.0     30.204000     63.061001   \n",
      "\n",
      "     ...  intan_ma    ppe_ma   sale_ma  cash_holdings_ma  roa_past  lev_past  \\\n",
      "11   ...  0.130887  0.009081  0.539652          0.527618  0.352416  0.000000   \n",
      "23   ...  0.066162  0.007559  0.360836          0.100509  0.039084  0.196057   \n",
      "35   ...  0.496920  0.087819  0.470287          0.056824  0.067262  0.014443   \n",
      "47   ...  0.062258  0.001563  0.337668          0.117395  0.569492  0.345433   \n",
      "59   ...  0.045939  0.005163  1.820588          0.032882  0.043842  0.090251   \n",
      "71   ...  0.039714  0.110957  1.664921          0.038581  0.035124  0.180192   \n",
      "83   ...  0.136929  0.003625  0.808372          0.059128  0.442118  0.087680   \n",
      "95   ...  0.632498  0.035856  0.416575          0.171175  0.115533  0.000000   \n",
      "107  ...  0.004504  0.012910  0.032015          0.051250  0.002998  0.315137   \n",
      "119  ...  0.014629  0.480603  0.822036          0.106419  0.206658  0.000000   \n",
      "\n",
      "     intan_past  ppe_past  sale_past  cash_holdings_past  \n",
      "11     0.194652  0.013346   0.534162            0.699963  \n",
      "23     0.052127  0.006567   0.171832            0.107148  \n",
      "35     0.564584  0.152826   0.469210            0.042710  \n",
      "47     0.065253  0.001596   0.331651            0.132563  \n",
      "59     0.043249  0.004455   1.713010            0.030002  \n",
      "71     0.037394  0.101637   1.630523            0.014571  \n",
      "83     0.133770  0.003621   0.812071            0.068696  \n",
      "95     0.626312  0.035433   0.421007            0.140181  \n",
      "107    0.004481  0.013339   0.033334            0.058877  \n",
      "119    0.016379  0.447505   0.790320            0.128360  \n",
      "\n",
      "[10 rows x 115 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_fe.csv', index_col=0)\n",
    "print(df_test.shape)\n",
    "print(df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998c86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = pd.read_excel('feature_ranking.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4939b485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi_score</th>\n",
       "      <th>sign_fscore</th>\n",
       "      <th>sign_fscore_0_1</th>\n",
       "      <th>corr</th>\n",
       "      <th>EN_coef</th>\n",
       "      <th>boruta_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rok</th>\n",
       "      <td>0.032073</td>\n",
       "      <td>1.179353e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.032669</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ta</th>\n",
       "      <td>0.582922</td>\n",
       "      <td>1.464884e-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267340</td>\n",
       "      <td>-1.404307e-07</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>0.633067</td>\n",
       "      <td>5.246456e-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.368732</td>\n",
       "      <td>1.466269e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pi</th>\n",
       "      <td>0.608157</td>\n",
       "      <td>8.614688e-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.299593</td>\n",
       "      <td>8.453656e-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>str</th>\n",
       "      <td>0.293955</td>\n",
       "      <td>1.578384e-46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mi_score   sign_fscore  sign_fscore_0_1      corr       EN_coef  \\\n",
       "rok  0.032073  1.179353e-01                0 -0.032669  0.000000e+00   \n",
       "ta   0.582922  1.464884e-03                1  0.267340 -1.404307e-07   \n",
       "txt  0.633067  5.246456e-13                1  0.368732  1.466269e-05   \n",
       "pi   0.608157  8.614688e-12                1  0.299593  8.453656e-06   \n",
       "str  0.293955  1.578384e-46                1  0.372870           NaN   \n",
       "\n",
       "     boruta_rank  \n",
       "rok           19  \n",
       "ta            49  \n",
       "txt            1  \n",
       "pi             3  \n",
       "str            9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a11114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr variable: ['rok', 'ta', 'txt', 'pi', 'str', 'xrd', 'ni', 'ppent', 'intant', 'dlc', 'dltt', 'capex', 'revenue', 'cce', 'adv', 'diff', 'roa', 'lev', 'intan', 'rd', 'ppe', 'sale', 'cash_holdings', 'adv_expenditure', 'capex2', 'cfc', 'dta', 'capex2_scaled', 'y_v2x_polyarchy', 'y_e_p_polity', 'y_BR_Democracy', 'WB_GDPgrowth', 'WB_GDPpc', 'WB_Inflation', 'rr_per_country', 'rr_per_sector', 'sektor_consumer discretionary', 'sektor_consumer staples', 'sektor_energy', 'sektor_health care', 'sektor_industrials', 'sektor_materials', 'sektor_real estate', 'sektor_technology', 'sektor_utilities', 'gielda_2', 'gielda_3', 'gielda_4', 'gielda_5', 'ta_log', 'txt_cat_(-63.011, -34.811]', 'txt_cat_(-34.811, 0.488]', 'txt_cat_(0.488, 24.415]', 'txt_cat_(24.415, 25.05]', 'txt_cat_(25.05, 308.55]', 'txt_cat_(308.55, 327.531]', 'txt_cat_(327.531, inf]', 'pi_cat_(-8975.0, -1.523]', 'pi_cat_(-1.523, 157.119]', 'pi_cat_(157.119, 465.9]', 'pi_cat_(465.9, 7875.5]', 'pi_cat_(7875.5, 8108.5]', 'pi_cat_(8108.5, inf]', 'str_cat_(0.0875, 0.192]', 'str_cat_(0.192, 0.28]', 'str_cat_(0.28, inf]', 'xrd_exists', 'ni_profit', 'ni_profit_20000', 'ppent_sqrt', 'intant_sqrt', 'dlc_cat_(42.262, 176.129]', 'dlc_cat_(176.129, 200.9]', 'dlc_cat_(200.9, inf]', 'dltt_cat_(39.38, 327.85]', 'dltt_cat_(327.85, 876.617]', 'dltt_cat_(876.617, inf]', 'capex_cat_(7.447, 79.55]', 'capex_cat_(79.55, 5451.0]', 'capex_cat_(5451.0, inf]', 'revenue_cat_(0.174, 1248.817]', 'revenue_cat_(1248.817, 4233.587]', 'revenue_cat_(4233.587, inf]', 'cce_cat_(5.619, 63.321]', 'cce_cat_(63.321, inf]', 'adv_cat_(0.3, 874.5]', 'adv_cat_(874.5, inf]', 'diff_positive', 'roa_clip', 'lev_sqrt', 'intan_pow2', 'rd_sqrt', 'ppe_clip', 'cash_holdings_sqrt', 'adv_expenditure_positive', 'diff_dta', 'cfc_dta', 'etr_y_past', 'etr_y_ma', 'diff_ma', 'roa_ma', 'lev_ma', 'intan_ma', 'ppe_ma', 'sale_ma', 'cash_holdings_ma', 'roa_past', 'lev_past', 'intan_past', 'ppe_past', 'sale_past', 'cash_holdings_past']\n",
      "df_train variable: ['Ticker', 'Nazwa2', 'rok', 'ta', 'txt', 'pi', 'str', 'xrd', 'ni', 'ppent', 'intant', 'dlc', 'dltt', 'capex', 'revenue', 'cce', 'adv', 'etr', 'diff', 'roa', 'lev', 'intan', 'rd', 'ppe', 'sale', 'cash_holdings', 'adv_expenditure', 'capex2', 'cfc', 'dta', 'capex2_scaled', 'y_v2x_polyarchy', 'y_e_p_polity', 'y_BR_Democracy', 'WB_GDPgrowth', 'WB_GDPpc', 'WB_Inflation', 'rr_per_country', 'rr_per_sector', 'sektor_consumer discretionary', 'sektor_consumer staples', 'sektor_energy', 'sektor_health care', 'sektor_industrials', 'sektor_materials', 'sektor_real estate', 'sektor_technology', 'sektor_utilities', 'gielda_2', 'gielda_3', 'gielda_4', 'gielda_5', 'ta_log', 'txt_cat_(-63.011, -34.811]', 'txt_cat_(-34.811, 0.488]', 'txt_cat_(0.488, 24.415]', 'txt_cat_(24.415, 25.05]', 'txt_cat_(25.05, 308.55]', 'txt_cat_(308.55, 327.531]', 'txt_cat_(327.531, inf]', 'pi_cat_(-8975.0, -1.523]', 'pi_cat_(-1.523, 157.119]', 'pi_cat_(157.119, 465.9]', 'pi_cat_(465.9, 7875.5]', 'pi_cat_(7875.5, 8108.5]', 'pi_cat_(8108.5, inf]', 'str_cat_(0.0875, 0.192]', 'str_cat_(0.192, 0.28]', 'str_cat_(0.28, inf]', 'xrd_exists', 'ni_profit', 'ni_profit_20000', 'ppent_sqrt', 'intant_sqrt', 'dlc_cat_(42.262, 176.129]', 'dlc_cat_(176.129, 200.9]', 'dlc_cat_(200.9, inf]', 'dltt_cat_(39.38, 327.85]', 'dltt_cat_(327.85, 876.617]', 'dltt_cat_(876.617, inf]', 'capex_cat_(7.447, 79.55]', 'capex_cat_(79.55, 5451.0]', 'capex_cat_(5451.0, inf]', 'revenue_cat_(0.174, 1248.817]', 'revenue_cat_(1248.817, 4233.587]', 'revenue_cat_(4233.587, inf]', 'cce_cat_(5.619, 63.321]', 'cce_cat_(63.321, inf]', 'adv_cat_(0.3, 874.5]', 'adv_cat_(874.5, inf]', 'diff_positive', 'roa_clip', 'lev_sqrt', 'intan_pow2', 'rd_sqrt', 'ppe_clip', 'cash_holdings_sqrt', 'adv_expenditure_positive', 'diff_dta', 'cfc_dta', 'etr_y_past', 'etr_y_ma', 'diff_ma', 'roa_ma', 'lev_ma', 'intan_ma', 'ppe_ma', 'sale_ma', 'cash_holdings_ma', 'roa_past', 'lev_past', 'intan_past', 'ppe_past', 'sale_past', 'cash_holdings_past']\n",
      "\n",
      "Only in fr: set()\n",
      "\n",
      "Only in df_train: {'etr', 'Nazwa2', 'Ticker'}\n"
     ]
    }
   ],
   "source": [
    "fr_transposed = fr.T\n",
    "\n",
    "print(\"fr variable:\", fr_transposed.columns.tolist())\n",
    "print(\"df_train variable:\", df_train.columns.tolist())\n",
    "\n",
    "fr_vars = set(fr_transposed.columns)\n",
    "df_train_vars = set(df_train.columns)\n",
    "\n",
    "print(\"\\nOnly in fr:\", fr_vars - df_train_vars)\n",
    "print(\"\\nOnly in df_train:\", df_train_vars - fr_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9731bb1",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96a7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Nazwa2', 'Ticker'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79471d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy()\n",
    "\n",
    "# 1. Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 2. Remove constant columns (zero variance)\n",
    "constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "df.drop(columns=constant_cols, inplace=True)\n",
    "\n",
    "# 3. Remove columns with too many missing values\n",
    "missing_ratio = df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > 0.80].index\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# 4. Simple outlier handling (winsorization)\n",
    "def winsorize(series, lower=0.01, upper=0.99):\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        q_low = series.quantile(lower)\n",
    "        q_high = series.quantile(upper)\n",
    "        return series.clip(q_low, q_high)\n",
    "    return series\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[col] = winsorize(df[col])\n",
    "\n",
    "# 5. Do not fill NaN for tree-based models (keep NaN)\n",
    "df = df.replace(\"nan\", np.nan)\n",
    "\n",
    "# 6. Convert suitable columns to datetime\n",
    "for col in df.columns:\n",
    "    if \"date\" in col.lower() or \"time\" in col.lower():\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='ignore')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 7. Convert object columns to category (for LightGBM/CatBoost)\n",
    "for col in df.select_dtypes(include=\"object\").columns:\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Output cleaned training set\n",
    "df_cleaned = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e884ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only in fr: {'y_BR_Democracy'}\n"
     ]
    }
   ],
   "source": [
    "df_cleaned_vars = set(df_cleaned.columns)\n",
    "print(\"\\nOnly in fr:\", fr_vars - df_cleaned_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7129aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = fr.drop(['y_BR_Democracy'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea77586",
   "metadata": {},
   "source": [
    "# Part 3: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ecc54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0 400 0.0 0.2050852271478223 1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. check column exists\n",
    "if 'etr' not in df_cleaned.columns:\n",
    "    raise KeyError(\"Column 'etr' not found in df_cleaned\")\n",
    "\n",
    "# 2. check for NaNs\n",
    "n_nan = df_cleaned['etr'].isna().sum()\n",
    "\n",
    "# 3. check for zeros (why MAPE -> inf)\n",
    "n_zero = (df_cleaned['etr'] == 0).sum()\n",
    "\n",
    "# 4. show a few diagnostics\n",
    "min_val = df_cleaned['etr'].min()\n",
    "q1 = df_cleaned['etr'].quantile(0.01)\n",
    "q50 = df_cleaned['etr'].median()\n",
    "q99 = df_cleaned['etr'].quantile(0.99)\n",
    "\n",
    "# Example print (you can remove prints if you don't want intermediate output)\n",
    "print(min_val, n_nan, n_zero, q1, q50, q99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b072be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae8f53ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_top20 0.5005981529794471 0.0439710136432261 20\n",
      "initial_top30 0.5005049600725819 0.046062942201379985 30\n",
      "initial_top50 0.5002790640439982 0.047948377870861994 50\n",
      "corr_top20 0.49704650899010155 0.04662191037056991 20\n",
      "AdaBoost_top20 0.5003003260607681 0.05186905721796153 20\n",
      "AdaBoost_top30 0.5005335251384061 0.04887290552326155 30\n",
      "GBM_top20 0.500213886309448 0.04682656386248585 20\n",
      "GBM_top30 0.49945146632041026 0.04808253951808071 30\n",
      "HistGBM_top20 0.5017056697711145 0.04597414196752422 20\n",
      "HistGBM_top30 0.5002458740950759 0.04937998415006714 30\n",
      "XGBoost_top20 0.5053481707393513 0.052739623468408846 20\n",
      "XGBoost_top30 0.5008455724601586 0.04692004588593411 30\n",
      "LightGBM_top20 0.4990701572724385 0.04882070087622782 20\n",
      "LightGBM_top30 0.5006190229618467 0.047778511267940076 30\n",
      "CatBoost_top20 0.5006033146554281 0.04515441257997822 20\n",
      "CatBoost_top30 0.5017075361441973 0.04867670275782283 30\n",
      "rfe_selected 0.5090307765929275 0.04561398978532554 13\n",
      "perm_top10 0.5040453471973592 0.046797812349456056 10\n",
      "BEST_FEATURE_SET: corr_top20\n",
      "BEST_sMAPE: 0.49704650899010155\n",
      "FEATURES: ['etr_y_ma', 'etr_y_past', 'str', 'txt', 'str_cat__0_0875__0_192_', 'str_cat__0_28__inf_', 'cfc', 'revenue', 'pi', 'intant', 'intant_sqrt', 'diff', 'revenue_cat__0_174__1248_817_', 'WB_GDPpc', 'xrd_exists', 'revenue_cat__4233_587__inf_', 'ta', 'ta_log', 'ni', 'rd']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# ============================================================\n",
    "# Step 0: Prepare data\n",
    "# ============================================================\n",
    "\n",
    "X = df_cleaned.drop(columns=['etr'], errors='ignore')\n",
    "y = df_cleaned['etr']\n",
    "\n",
    "# clean col names\n",
    "X.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', str(col)) for col in X.columns]\n",
    "fr.index = [re.sub(r'[^a-zA-Z0-9_]', '_', str(idx)) for idx in fr.index]\n",
    "\n",
    "# normalize fr\n",
    "fr = fr.fillna(0)\n",
    "fr_norm = fr.copy()\n",
    "for col in fr_norm.columns:\n",
    "    col_min = fr_norm[col].min()\n",
    "    col_max = fr_norm[col].max()\n",
    "    if col_max - col_min == 0:\n",
    "        fr_norm[col] = 0\n",
    "    else:\n",
    "        fr_norm[col] = (fr_norm[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "fr_norm['composite_score'] = fr_norm.mean(axis=1)\n",
    "fr_sorted = fr_norm.sort_values(by='composite_score', ascending=False)\n",
    "\n",
    "# initial feature sets\n",
    "all_feature_sets = {}\n",
    "all_feature_sets['initial_top20'] = fr_sorted.index[:20].tolist()\n",
    "all_feature_sets['initial_top30'] = fr_sorted.index[:30].tolist()\n",
    "all_feature_sets['initial_top50'] = fr_sorted.index[:50].tolist()\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Feature sets from correlation\n",
    "# ============================================================\n",
    "\n",
    "if 'corr' in fr.columns:\n",
    "    fr['corr_abs'] = np.abs(fr['corr'])\n",
    "    corr_features = fr.sort_values('corr_abs', ascending=False).index[:20].tolist()\n",
    "    all_feature_sets['corr_top20'] = corr_features\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Tree Models Feature Importance\n",
    "# ============================================================\n",
    "\n",
    "tree_models = {\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'GBM': GradientBoostingRegressor(random_state=42),\n",
    "    'HistGBM': HistGradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostRegressor(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "def time_aware_permutation_importance(model, X, y):\n",
    "    baseline_pred = model.predict(X)\n",
    "    baseline_error = np.mean((baseline_pred - y) ** 2)\n",
    "    importances = []\n",
    "    for col in X.columns:\n",
    "        X_perm = X.copy()\n",
    "        X_perm[col] = X_perm[col].shift(1).fillna(method=\"bfill\")\n",
    "        pred = model.predict(X_perm)\n",
    "        error = np.mean((pred - y) ** 2)\n",
    "        importances.append(error - baseline_error)\n",
    "    return np.array(importances)\n",
    "\n",
    "valid_top50 = [f for f in all_feature_sets['initial_top50'] if f in X.columns]\n",
    "\n",
    "for name, model in tree_models.items():\n",
    "    if len(valid_top50) == 0:\n",
    "        continue\n",
    "    \n",
    "    X_train = X[valid_top50]\n",
    "    model.fit(X_train, y)\n",
    "\n",
    "    if name == 'CatBoost':\n",
    "        fi = model.get_feature_importance()\n",
    "    elif name == 'HistGBM':\n",
    "        fi = time_aware_permutation_importance(model, X_train, y)\n",
    "    else:\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            fi = model.feature_importances_\n",
    "        else:\n",
    "            fi = time_aware_permutation_importance(model, X_train, y)\n",
    "\n",
    "    fi_series = pd.Series(fi, index=valid_top50).sort_values(ascending=False)\n",
    "    all_feature_sets[f'{name}_top20'] = fi_series.index[:20].tolist()\n",
    "    all_feature_sets[f'{name}_top30'] = fi_series.index[:30].tolist()\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: RFE\n",
    "# ============================================================\n",
    "\n",
    "rfe_candidates = valid_top50\n",
    "\n",
    "if len(rfe_candidates) >= 10:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X[rfe_candidates])\n",
    "    estimator = LinearRegression()\n",
    "    selector = RFECV(estimator, step=1, cv=5, min_features_to_select=5)\n",
    "    selector.fit(X_scaled, y.values.ravel())\n",
    "\n",
    "    rfe_features = [rfe_candidates[i] for i in range(len(rfe_candidates)) if selector.support_[i]]\n",
    "    all_feature_sets['rfe_selected'] = rfe_features\n",
    "\n",
    "# ============================================================\n",
    "# Step 4: Permutation importance on top20\n",
    "# ============================================================\n",
    "\n",
    "if 'initial_top20' in all_feature_sets:\n",
    "    perm_features = [f for f in all_feature_sets['initial_top20'] if f in X.columns]\n",
    "    if len(perm_features) > 5:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X[perm_features])\n",
    "        estimator = LinearRegression()\n",
    "        estimator.fit(X_scaled, y.values.ravel())\n",
    "\n",
    "        X_df_scaled = pd.DataFrame(X_scaled, columns=perm_features)\n",
    "        importances = time_aware_permutation_importance(estimator, X_df_scaled, y)\n",
    "        sorted_idx = np.argsort(importances)[::-1]\n",
    "        all_feature_sets['perm_top10'] = [perm_features[i] for i in sorted_idx[:10]]\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Time-series CV (sMAPE)\n",
    "# ============================================================\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "def time_series_cv(X, y, model, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        scores.append(smape(y_test, y_pred))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# ============================================================\n",
    "# Step 6: Evaluate all feature sets\n",
    "# ============================================================\n",
    "\n",
    "benchmark_model = GradientBoostingRegressor(random_state=42)\n",
    "feature_set_performance = {}\n",
    "\n",
    "for set_name, features in all_feature_sets.items():\n",
    "    valid_features = [f for f in features if f in X.columns]\n",
    "    if len(valid_features) < 3:\n",
    "        continue\n",
    "\n",
    "    X_subset = X[valid_features]\n",
    "    mean_s, std_s = time_series_cv(X_subset, y, benchmark_model)\n",
    "\n",
    "    feature_set_performance[set_name] = {\n",
    "        'sMAPE_mean': mean_s,\n",
    "        'sMAPE_std': std_s,\n",
    "        'n_features': len(valid_features),\n",
    "        'features': valid_features\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Step 7: Print clean final output only\n",
    "# ============================================================\n",
    "\n",
    "for name, metrics in feature_set_performance.items():\n",
    "    print(name, metrics['sMAPE_mean'], metrics['sMAPE_std'], metrics['n_features'])\n",
    "\n",
    "# Best set\n",
    "best_set_name = min(feature_set_performance, key=lambda k: feature_set_performance[k]['sMAPE_mean'])\n",
    "best_info = feature_set_performance[best_set_name]\n",
    "\n",
    "print(\"BEST_FEATURE_SET:\", best_set_name)\n",
    "print(\"BEST_sMAPE:\", best_info['sMAPE_mean'])\n",
    "print(\"FEATURES:\", best_info['features'])\n",
    "\n",
    "feature_selection_results = {\n",
    "    'best_set_name': best_set_name,\n",
    "    'best_features': best_info['features'],\n",
    "    'best_performance': best_info\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce18443",
   "metadata": {},
   "source": [
    "# Part 3: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "753da9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Starting XGBoostLSS training...\n",
      "[DEBUG] XGBoostLSS imports successful\n",
      "[DEBUG] Testing 15 parameter combinations\n",
      "[DEBUG] Combo 1/15: New best RMSE = 0.145664\n",
      "[DEBUG] Combo 11 failed: normal expects all elements of std >= 0.0\n",
      "[DEBUG] Combo 13/15: New best RMSE = 0.145120\n",
      "[DEBUG] XGBoostLSS training completed! Best RMSE: 0.145120\n",
      "\n",
      "======================================================================\n",
      "FINAL HYPERPARAMETER TUNING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Training Data: (3993, 20)\n",
      "Features: 20\n",
      "Time Series CV: 5 splits\n",
      "\n",
      "LOCAL CHAMPIONS - VALIDATION RMSE:\n",
      "--------------------------------------------------\n",
      " 1. XGBoostLSS      RMSE: 0.145120\n",
      " 2. LightGBM        RMSE: 0.145436\n",
      " 3. CatBoost        RMSE: 0.145963\n",
      " 4. XGBoost         RMSE: 0.146174\n",
      " 5. GBM             RMSE: 0.147862\n",
      " 6. AdaBoost        RMSE: 0.148792\n",
      " 7. HistGBM         RMSE: 0.149292\n",
      "\n",
      "======================================================================\n",
      "OVERALL BEST MODEL\n",
      "======================================================================\n",
      "Algorithm: XGBoostLSS\n",
      "Validation RMSE: 0.145120\n",
      "\n",
      "Total models trained: 7\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare training data with best feature set\n",
    "X_train = X[feature_selection_results['best_features']]\n",
    "y_train = y\n",
    "\n",
    "# Step 2: Define rolling window cross-validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Step 3: Define all 7 models with hyperparameter grids\n",
    "param_grids = {\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "        'loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'GBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'HistGBM': {\n",
    "        'max_iter': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10],\n",
    "        'l2_regularization': [0, 0.1, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1.0],\n",
    "        'reg_lambda': [1, 1.5, 2.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7, -1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1.0],\n",
    "        'reg_lambda': [1, 1.5, 2.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [4, 6, 8],\n",
    "        'l2_leaf_reg': [1, 3, 5]\n",
    "    },\n",
    "    'XGBoostLSS': {\n",
    "        'num_boost_round': [100, 200],\n",
    "        'eta': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'subsample': [0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'GBM': GradientBoostingRegressor(random_state=42),\n",
    "    'HistGBM': HistGradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n",
    "}\n",
    "\n",
    "# Step 4: Hyperparameter tuning with Rolling Window CV\n",
    "local_champions = {}\n",
    "tuning_results = {}\n",
    "\n",
    "# Train standard models (all except XGBoostLSS)\n",
    "for name, model in models.items():\n",
    "    search = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_grids[name],\n",
    "        n_iter=15,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    local_champions[name] = search.best_estimator_\n",
    "    tuning_results[name] = {\n",
    "        'best_params': search.best_params_,\n",
    "        'best_rmse': np.sqrt(-search.best_score_)\n",
    "    }\n",
    "\n",
    "# Special handling for XGBoostLSS\n",
    "print(\"\\n[DEBUG] Starting XGBoostLSS training...\")\n",
    "xgblss_trained = False\n",
    "xgblss_error = None\n",
    "\n",
    "try:\n",
    "    from xgboostlss.model import XGBoostLSS\n",
    "    from xgboostlss.distributions.Gaussian import Gaussian\n",
    "    import xgboost as xgb\n",
    "    print(\"[DEBUG] XGBoostLSS imports successful\")\n",
    "    \n",
    "    best_rmse = np.inf\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    # Random search for XGBoostLSS (sample 15 combinations)\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \n",
    "    all_combinations = []\n",
    "    for num_rounds in param_grids['XGBoostLSS']['num_boost_round']:\n",
    "        for eta in param_grids['XGBoostLSS']['eta']:\n",
    "            for depth in param_grids['XGBoostLSS']['max_depth']:\n",
    "                for subsample in param_grids['XGBoostLSS']['subsample']:\n",
    "                    all_combinations.append({\n",
    "                        'num_boost_round': num_rounds,\n",
    "                        'eta': eta,\n",
    "                        'max_depth': depth,\n",
    "                        'subsample': subsample\n",
    "                    })\n",
    "    \n",
    "    sampled_combinations = random.sample(all_combinations, min(15, len(all_combinations)))\n",
    "    print(f\"[DEBUG] Testing {len(sampled_combinations)} parameter combinations\")\n",
    "    \n",
    "    for idx, params_combo in enumerate(sampled_combinations, 1):\n",
    "        params_dict = {\n",
    "            'eta': params_combo['eta'],\n",
    "            'max_depth': params_combo['max_depth'],\n",
    "            'subsample': params_combo['subsample']\n",
    "        }\n",
    "        \n",
    "        rmse_scores = []\n",
    "        try:\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "                X_fold_train = X_train.iloc[train_idx]\n",
    "                X_fold_val = X_train.iloc[val_idx]\n",
    "                y_fold_train = y_train.iloc[train_idx]\n",
    "                y_fold_val = y_train.iloc[val_idx]\n",
    "                \n",
    "                # Convert to numpy arrays to ensure compatibility\n",
    "                X_fold_train_np = X_fold_train.values if hasattr(X_fold_train, 'values') else X_fold_train\n",
    "                X_fold_val_np = X_fold_val.values if hasattr(X_fold_val, 'values') else X_fold_val\n",
    "                y_fold_train_np = y_fold_train.values if hasattr(y_fold_train, 'values') else y_fold_train\n",
    "                y_fold_val_np = y_fold_val.values if hasattr(y_fold_val, 'values') else y_fold_val\n",
    "                \n",
    "                dtrain = xgb.DMatrix(X_fold_train_np, label=y_fold_train_np)\n",
    "                dval = xgb.DMatrix(X_fold_val_np, label=y_fold_val_np)\n",
    "                \n",
    "                model_temp = XGBoostLSS(\n",
    "                    Gaussian(stabilization=\"None\", \n",
    "                           response_fn=\"exp\", \n",
    "                           loss_fn=\"nll\")\n",
    "                )\n",
    "                \n",
    "                model_temp.train(\n",
    "                    params=params_dict,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=params_combo['num_boost_round'],\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                \n",
    "                pred_params = model_temp.predict(dval, pred_type=\"parameters\")\n",
    "                y_pred = pred_params['loc']\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_fold_val_np, y_pred))\n",
    "                rmse_scores.append(rmse)\n",
    "            \n",
    "            mean_rmse = np.mean(rmse_scores)\n",
    "            \n",
    "            if mean_rmse < best_rmse:\n",
    "                best_rmse = mean_rmse\n",
    "                best_params = params_combo.copy()\n",
    "                \n",
    "                # Train final model with best params\n",
    "                X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "                y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "                \n",
    "                best_model = XGBoostLSS(\n",
    "                    Gaussian(stabilization=\"None\", \n",
    "                           response_fn=\"exp\", \n",
    "                           loss_fn=\"nll\")\n",
    "                )\n",
    "                dtrain_full = xgb.DMatrix(X_train_np, label=y_train_np)\n",
    "                best_model.train(\n",
    "                    params=params_dict,\n",
    "                    dtrain=dtrain_full,\n",
    "                    num_boost_round=params_combo['num_boost_round'],\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                print(f\"[DEBUG] Combo {idx}/{len(sampled_combinations)}: New best RMSE = {best_rmse:.6f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] Combo {idx} failed: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    if best_model is not None:\n",
    "        local_champions['XGBoostLSS'] = best_model\n",
    "        tuning_results['XGBoostLSS'] = {\n",
    "            'best_params': best_params,\n",
    "            'best_rmse': best_rmse\n",
    "        }\n",
    "        xgblss_trained = True\n",
    "        print(f\"[DEBUG] XGBoostLSS training completed! Best RMSE: {best_rmse:.6f}\")\n",
    "    else:\n",
    "        print(\"[DEBUG] XGBoostLSS: No valid model trained\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    xgblss_error = f\"Import Error: {str(e)}\"\n",
    "    print(f\"[DEBUG] {xgblss_error}\")\n",
    "except Exception as e:\n",
    "    xgblss_error = f\"Runtime Error: {type(e).__name__}: {str(e)}\"\n",
    "    print(f\"[DEBUG] {xgblss_error}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Step 5: Display final results only\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTraining Data: {X_train.shape}\")\n",
    "print(f\"Features: {len(feature_selection_results['best_features'])}\")\n",
    "print(f\"Time Series CV: {tscv.n_splits} splits\")\n",
    "\n",
    "print(\"\\nLOCAL CHAMPIONS - VALIDATION RMSE:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sorted_models = sorted(tuning_results.items(), key=lambda x: x[1]['best_rmse'])\n",
    "\n",
    "for rank, (name, results) in enumerate(sorted_models, 1):\n",
    "    print(f\"{rank:2d}. {name:15} RMSE: {results['best_rmse']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OVERALL BEST MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_model_name = sorted_models[0][0]\n",
    "best_model_rmse = sorted_models[0][1]['best_rmse']\n",
    "\n",
    "print(f\"Algorithm: {best_model_name}\")\n",
    "print(f\"Validation RMSE: {best_model_rmse:.6f}\")\n",
    "\n",
    "print(f\"\\nTotal models trained: {len(local_champions)}\")\n",
    "\n",
    "if not xgblss_trained and xgblss_error:\n",
    "    print(f\"\\nNote: XGBoostLSS not trained - {xgblss_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88ff2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../Advanced Ensemble Methods/models', exist_ok=True)\n",
    "\n",
    "# Save each local champion model\n",
    "for name, model in local_champions.items():\n",
    "    filename = f'../Advanced Ensemble Methods/models/{name.lower()}_local_champion.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save tuning results\n",
    "with open('../Advanced Ensemble Methods/models/tuning_results.pkl', 'wb') as f:\n",
    "    pickle.dump(tuning_results, f)\n",
    "\n",
    "# Save feature selection results\n",
    "with open('../Advanced Ensemble Methods/models/feature_selection_results.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_selection_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
